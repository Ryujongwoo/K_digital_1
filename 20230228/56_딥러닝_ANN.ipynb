{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bd035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1e9ba",
   "metadata": {},
   "source": [
    "인공 신경망(Artifical Neural Network, ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de3e68",
   "metadata": {},
   "source": [
    "신경망의 개념  \n",
    "다층 퍼셉트론(multi-layer perceptron)은 이름 그대로 퍼셉트론의 층이 여러 개라는 뜻으로 다층 퍼셉트론은 기존의 데이터 공간을 변형함으로써 기존의 하나의 퍼셉트론으로 해결할 수 없었던 문제를 해결할 수 있게 되는 것이다. 이처럼 다수의 뉴런을 사용해서 만든 것을 인공 신경망이라고 하며 줄여서 신경망이라고 부른다.\n",
    "\n",
    "<img src=\"ann01.png\" width=\"500\"/>\n",
    "\n",
    "입력층(input layer)에 입력된 데이터는 출력층(output layer)에 도달하기 전에 은닉층(hidden layer)이라는 층을 거친 후 출력층에 도달한다. 은닉층은 하나의 층뿐만 아니라 다수의 층으로 정할 수 있고 딥러닝(Deep Learning)이라는 이름은 이 은닉층의 깊이가 깊다는 뜻에서 나온 이름이다. 입력층의 노드 개수는 피쳐 개수와 동일하고 출력츨의 노드 개수는 분류하려는 클래스의 수와 같다.\n",
    "\n",
    "출력층의 각 노드가 나타내는 수는 해당 클래스에 대한 스코어이며 스코어가 높을수록 해당 클래스에 속할 확률이 높다는 의미이고 최종적으로 스코어가 가장 높은 클래스를 선정한다.\n",
    "\n",
    "신경망 함수는 $f(x) = f_3(f_2(f_1(x)))$와 같은 합성 함수 형태로 표현된다. $f_1$을 첫 번째 신경망 층, $f_2$를 두 번째 신경망 층, $f_3$를 세 번째 신경망 층 이라고 생각하면 된다. 신경망 층이 깊을수록 함수의 개수는 많아지고 합성 함수도 복잡해진다. 이때 합성 함수의 개수는 모형의 깊이(depth)를 의미하는데, 딥러닝에서 딥(deep)이라는 용어는 바로 여기서 나온 용어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1c4d4f",
   "metadata": {},
   "source": [
    "오차 역전파(back propagation)  \n",
    "다층 퍼셉트론에서 최적값을 찾아가는 과정은 오차 역전파 방법을 사용한다.\n",
    "\n",
    "<img src=\"ann02.png\" width=\"500\"/>\n",
    "\n",
    "입력층 - 은닉층 - 출력층 순서대로 흘러가는 것을 순전파(forward propagation)라고 하고 반대로 역전파는 출력층 - 은닉층 - 입력층 순서대로 반대로 거슬러 올라가는 방향이다. 오차 역전파를 통해 오차를 기반으로 가중치를 수정한 후 더 좋은 성능을 낼 수 있도록 모델을 개선한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b157ba2",
   "metadata": {},
   "source": [
    "활성화 함수  \n",
    "활성화 함수는 딥러닝에서 입력값과 가중치, 편향을 계산해서 해당 노드를 활성화할지를 결정하는 함수이다. 딥러닝에서는 결과값을 결정하는 여러 가지 활성화 함수가 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038bafb",
   "metadata": {},
   "source": [
    "■ 계단 함수(step function)  \n",
    "계산 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \n",
    "\\begin{cases}\n",
    "0, x \\leq 0 \\\\\n",
    "1, x > 0\n",
    "\\end{cases}$$\n",
    "\n",
    "계단 함수는 입력값이 0 이하일 경우에는 0을 출력하고, 0을 초과할 때만 1을 출력한다. 계단 함수의 출력값은 0 또는 1로 오직 두 가지 값만 가진다는 특징이 있다.\n",
    "\n",
    "<img src=\"ann03.png\" width=\"400\"/>\n",
    "\n",
    "입력값 x가 0을 기준으로 0을 넘기 전과 후의 값에 따라 출려값이 바뀌는 것을 알 수 있다. 계단 함수는 사용하기 간단하다는 장점이 있지만 미분이 가능하지 않다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41274f",
   "metadata": {},
   "source": [
    "■ 부호 함수(sign function)  \n",
    "부호 함수는 아래의 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \n",
    "\\begin{cases}\n",
    "1, x > 0 \\\\\n",
    "0, x = 0 \\\\\n",
    "-1, x < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "부호 함수를 그림으로 나타내면 아래와 같다.\n",
    "\n",
    "<img src=\"ann04.png\" width=\"400\"/>\n",
    "\n",
    "계단 함수와 비슷하게 생겼지만, 0 또는 1 값만 출력하는 계단 함수와는 달리 -1, 0, 1 값을 출력하는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8cef15",
   "metadata": {},
   "source": [
    "■ 시그모이드 함수(sigmoid function)  \n",
    "시그모이드 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "시그모이드 함수를 그림으로 나타내면 아래와 같다.\n",
    "\n",
    "<img src=\"ann05.png\" width=\"400\"/>\n",
    "\n",
    "시그모이드 함수는 0과 1 사이의 값을 출력한다.  \n",
    "시그모이드 함수를 딥러닝에 적용했을 때 단점은 학습하는 과정에서 미분을 반복하면 그래디언트 값이 매우 작아지는 그래디언트 소실 문제(vanishing gradient problem)가 발생할 수 있다는 것이다. 시그모이드 함수에서 $x$ 값이 지나치게 크거나 작을 경우, 미분값이 0에 가까워지고, 이는 그래디언트 소실 문제를 발생시켜 학습 속도가 느려질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb3f0d9",
   "metadata": {},
   "source": [
    "■ 하이퍼볼릭 탄젠트 함수(hyperbolic tangent function, tanh)  \n",
    "하이퍼볼릭 탄젠트 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \\frac{exp(x) - exp(x)}{exp(x) + exp(x)}$$\n",
    "\n",
    "시그모이드 함수의 범위는 0부터 1 사이인 반면에 하이퍼볼릭 탄젠트 함수의 범위는 -1부터 1 사이이다.\n",
    "\n",
    "<img src=\"ann06.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf26b8",
   "metadata": {},
   "source": [
    "■ 렐루 함수(ReLU, Rectified Linear function)  \n",
    "렐루 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \n",
    "\\begin{cases}\n",
    "0, x \\leq 0 \\\\\n",
    "x, x > 0\n",
    "\\end{cases}$$\n",
    "\n",
    "위 식은 아래와 같이 한 줄로도 표현할 수 있다.\n",
    "\n",
    "$$\\phi(x) = max(x, 0)$$\n",
    "\n",
    "렐루 함수를 그림으로 나타내면 아래와 같다.\n",
    "\n",
    "<img src=\"ann07.png\" width=\"500\"/>\n",
    "\n",
    "렐루 함수는 $x$ 값이 0 이하라면 0을 출력하고, 양수이면 $x$ 값을 그대로 출력한다. 렐루 함수는 계단 함수, 부호 함수, 시그모이드, 하이퍼볼릭 탄젠트 함수와는 다르게 출력값의 상한선이 없다는 특징이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1208d4a",
   "metadata": {},
   "source": [
    "■ 리키 렐루 함수(Leaky ReLU function)  \n",
    "리키 렐루 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \n",
    "\\begin{cases}\n",
    "x, x > 0 \\\\\n",
    "ax, x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "위 함수에서 $a \\leq 1$ 이라면 아래와 같이 쓸 수 있다.\n",
    "\n",
    "$$\\phi(x) = max(x, ax)$$\n",
    "\n",
    "리키 렐루 함수를 그림으로 나타내면 아래와 같다.\n",
    "\n",
    "<img src=\"ann08.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed31a5a",
   "metadata": {},
   "source": [
    "■ 항등 함수(identity function)  \n",
    "항등 함수 또는 선형 함수(linear function)라고도 부르는 이 함수는 입력과 출력이 같은 값을 가진다. 주로 회귀 문제에서의 출력층 활성화 함수로 사용된다.  \n",
    "항등 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = x$$\n",
    "\n",
    "항등 함수를 그림으로 나타내면 아래와 같다.\n",
    "\n",
    "<img src=\"ann09.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0ee46",
   "metadata": {},
   "source": [
    "■ 소프트맥스 함수(softmax function)  \n",
    "소프트맥스 함수는 주로 최종 출력층에 사용되는 활성화 함수이다. 만약 해결해야 할 문제가 회귀 문제라면 출력층에 항등 함수를 쓰고, 분류 문제일 경우에는 소프트맥스 함수를 사용한다. 소프트맥스 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \\frac {exp(x_k)}{\\sum_{i=1}^n exp(x_i)}$$\n",
    "\n",
    "소프트맥스 함수를 사용할 때 위 식을 그대로 사용할 경우 오버플로우 문제가 발생할 수 있다. 오버플로우는 출력값이 컴퓨터가 표현할 수 있는 수의 한계를 초과하는 문제를 말한다. 예를 들어 $x_k = 1000$일 때, $exp(1000)$는 매우 큰 수가 되므로 계산하는 것이 불가능하다. 이 문제를 해결하기 위해 소프트맥스 함수를 아래와 같이 변형해서 쓰기도 한다.\n",
    "\n",
    "$$\\phi(x) = \\frac {exp(x_k) + C}{\\sum_{i=1}^n exp(x_i) + C}$$\n",
    "\n",
    "소프트맥스 함수에서 지수 연산을 할 때, 입력값에 임의 상수 $C$를 더하거나 빼서 오버프로우 문제를 해결한다. 임의의 상수 $C$는 일반적으로 입력값의 최대값을 이용한다. 입력값이 999, 1000, 1001이라 하면 입력값의 최대값은 1001이므로 임의의 상수 $C$는 1001이 된다. 기존에는 $exp(999)$, $exp(1000)$, $exp(1001)$을 구해야 해서 오버프롤우가 발생살 수 있지만, 입력값의 최대값을 빼주면 $exp(-2)$, $exp(-1)$, $exp(0)$과 같이 연산이 편해진다.\n",
    "\n",
    "<img src=\"ann10.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55eee13",
   "metadata": {},
   "source": [
    "배치 정규화(batch normalization)  \n",
    "배치 정규화는 해당층의 값의 분포를 변경하는 방법으로, 평균과 분산을 고정시키는 방법이다. 배치 정규화를 이용하면 그래디언트 소실 줄임을로써 신경망의 학습 속도를 향상시키 수 있다는 장점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059df44",
   "metadata": {},
   "source": [
    "드롭아웃(dropout)  \n",
    "드롭아웃은 신경망의 모든 노드를 사용하는 것이 아닌, 일부 노드를 사용하는 방법이다.\n",
    "\n",
    "<img src=\"ann11.png\" width=\"700\"/>\n",
    "\n",
    "왼쪽 그림은 드롭아웃을 적용하기 전 신경망이고, 오른쪽 그림이 드롭아웃을 적용한 신경망이다.\n",
    "\n",
    "드롭아웃은 신경망에서 노드를 일시적으로 제거하는 방법이다. 어떤 노드를 신경망에서 제거할지는 각 층에서 무작위로 선택된다. 드롭아웃을 적용하면 신경망의 노드 숫자가 줄어들고 이에 따라 연산량도 줄어들고 오버피팅을 방지할 수 있다는 장점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd075c34",
   "metadata": {},
   "source": [
    "텐서플로우(Tensorflow) 2.x 소개  \n",
    "텐서플로우는 파이썬을 이용해 딥러닝 학습 시 사용하는 라이브러리이다. 텐서플로우를 활용하면 신경망을 기본으로 하는 딥러닝에서 다양한 신경망 관련 연산을 처리할 수 있어 편리하다.\n",
    "\n",
    "텐서플로우를 이용해 신경망 구조를 만드는 방법은 크게 시퀀스 API를 사용하는 방법과 함수형 API를 사용하는 방법이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6a5a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32, 32, 100)       200       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32, 32, 50)        5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32, 32, 5)         255       \n",
      "=================================================================\n",
      "Total params: 5,505\n",
      "Trainable params: 5,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 시퀀스 API를 사용하는 방법\n",
    "# 텐서플로우에서 제공하는 Sequential()를 선언 후 시퀀스 모델에 add() 함수로 추가함으로써 실제 layer를 쌓는다.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# units 옵션으로 layer의 출력 개수를 지정하고 activation 옵션으로 활성화 함수를 지정해서 layer를 만든다.\n",
    "model.add(Dense(units=100, activation='relu', input_shape=(32, 32, 1))) # 입력 layer\n",
    "model.add(Dense(units=50, activation='relu')) # 히든 layer\n",
    "model.add(Dense(units=5, activation='softmax')) # 출력 layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e81f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 1)]       0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32, 32, 100)       200       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32, 32, 50)        5050      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32, 32, 5)         255       \n",
      "=================================================================\n",
      "Total params: 5,505\n",
      "Trainable params: 5,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 함수형 API를 사용하는 방법\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(32, 32, 1))\n",
    "x = Dense(units=100, activation='relu')(input_layer) # 입력 layer\n",
    "x = Dense(units=50, activation='relu')(x) # 히든 layer\n",
    "# 위 코드는 은닉층과 활성화 함수를 분리해서 아래와 같이 쓸 수 있다.\n",
    "# x = Dense(units=50)(x)\n",
    "# x = Activation('relu')(x)\n",
    "output_layer = Dense(units=5, activation='softmax')(x) # 출력 layer\n",
    "model2 = Model(input_layer, output_layer)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4436430a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# 모형 저장 및 불러오기\n",
    "# 모형을 저장하고 나중에 모형을 불러와서 다시 사용할 수 있다.\n",
    "# 모델 저장은 save() 함수를 사용하고 h5 파일로 저장한다. h5 파일은 hdf5 파일을 의미하는데 hdf5는 Hierachical Data\n",
    "# Format version 5의 줄인말로 대용량 데이터를 저장하기 위한 파일 포맷이다.\n",
    "model.save('ann_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "106a1ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32, 32, 100)       200       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32, 32, 50)        5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32, 32, 5)         255       \n",
      "=================================================================\n",
      "Total params: 5,505\n",
      "Trainable params: 5,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "ann_model = load_model('ann_model.h5')\n",
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef91d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
