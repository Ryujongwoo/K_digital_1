{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e059e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c1ac5",
   "metadata": {},
   "source": [
    "<img src=\"./MNIST.png\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d5bc0",
   "metadata": {},
   "source": [
    "MNIST 손글씨 실습을 위해서 케라스에서 제공하는 MNIST 데이터셋을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1200eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f52c5e",
   "metadata": {},
   "source": [
    "학습 데이터에는 총 60,000개의 샘플이 있고 테스트 데이터에는 총 10,000개의 샘플이 있다.  \n",
    "MNIST 데이터는 이미지 하나가 28개의 행과 28개의 열을 갖는 픽셀 데이터이다. 각 픽셀은 흑백 사진과 같이 0부터 255까지의 그레이스케일을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53069c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 28, 28), y_train.shape: (60000,)\n",
      "x_test.shape: (10000, 28, 28), y_test.shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train.shape: {}, y_train.shape: {}'.format(x_train.shape, y_train.shape)) # 학습 데이터\n",
    "print('x_test.shape: {}, y_test.shape: {}'.format(x_test.shape, y_test.shape)) # 테스트 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e84671",
   "metadata": {},
   "source": [
    "학습 데이터를 학습 데이터(5만개)와 검증 데이터(1만개)로 분리한다.  \n",
    "학습 중간마다 검증 데이터로 모델의 성능을 측정하면 모델 학습이 제대로 진행되는지 검증 정확도를 알 수 있고 학습 정확도는 올라가는데 검증 정확도가 더이상 올라가지 않거나 떨어질 경우 조기 종료를 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7832be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (50000, 28, 28), y_train.shape: (50000,)\n",
      "x_val.shape: (10000, 28, 28), y_val.shape: (10000,)\n",
      "x_test.shape: (10000, 28, 28), y_test.shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_val = x_train[50000:] # 28행 28열로 구성된 검증 데이터\n",
    "x_train = x_train[:50000] # 28행 28열로 구성된 학습 데이터\n",
    "y_val = y_train[50000:] # 검증 데이터 실제값\n",
    "y_train = y_train[:50000] # 학습 데이터 실제값\n",
    "print('x_train.shape: {}, y_train.shape: {}'.format(x_train.shape, y_train.shape)) # 학습 데이터\n",
    "print('x_val.shape: {}, y_val.shape: {}'.format(x_val.shape, y_val.shape)) # 검증 데이터\n",
    "print('x_test.shape: {}, y_test.shape: {}'.format(x_test.shape, y_test.shape)) # 테스트 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4530acc3",
   "metadata": {},
   "source": [
    "학습 데이터를 출력해보면 데이터가 0부터 255까지의 숫자(그레이스케일)로 구성된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f9604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  51 159 253 159  50   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252 253 122   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228  47  79 255 168   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0   0   0   0   0 253 252 165   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0  57 252 252  63   0   0   0   0   0   0   0   0   0 253 252 195   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253 196   0   0   0   0   0 \n",
      "  0   0   0   0   0   0  76 246 252 112   0   0   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0 \n",
      "  0   0   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135 253 186  12   0   0   0   0   0 \n",
      "  0   0   0   0   0   0  85 252 223   0   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165 252 173   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0  86 253 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253 223 167  56   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0  28 199 252 252 253 252 252 233 145   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])\n",
    "for i in x_train[1]:\n",
    "    for j in i:\n",
    "        print('{:3d} '.format(j), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac3430",
   "metadata": {},
   "source": [
    "MNIST 데이터를 불러왔으니 다층 퍼셉트론의 입력값으로 들어갈 수 있도록 numpy의 reshape() 함수를 사용해서 2차원 배열 형태의 데이터를 1차원 배열 형태로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67107635",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (50000, 28, 28)\n",
      "x_train.shape: (50000, 784)\n",
      "x_val.shape: (10000, 784)\n",
      "x_test.shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 28 * 28 픽셀의 단색 이미지이므로 데이터 형태를 784개의 1차원 배열 형태로 변환한다.\n",
    "print('x_train.shape: {}'.format(x_train.shape))\n",
    "# x_train = x_train.reshape(50000, 784)\n",
    "x_train = np.reshape(x_train, [50000, 784]) # 28행 28열로 구성된 학습 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "print('x_train.shape: {}'.format(x_train.shape))\n",
    "x_val = np.reshape(x_val, [10000, 784]) # 28행 28열로 구성된 검증 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "print('x_val.shape: {}'.format(x_val.shape))\n",
    "x_test = np.reshape(x_test, [10000, 784]) # 28행 28열로 구성된 테스트 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "print('x_test.shape: {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc7f0f",
   "metadata": {},
   "source": [
    "1차원으로 변경된 데이터를 그대로 다층 퍼셉트론에 입력해도 되지만 조금 더 효율적인 학습을 위해 데이터를 정규화 시킨다. 정규화는 모델의 학습 시간을 단축시키고 더 나은 성능을 보이게 하는 효과가 있다.  \n",
    "MNIST 데이터의 모든 값들은 0부터 255까지의 범위 안에 있으므로 255로 나눠 모든 값들을 0부터 1사이의 값으로 정규화 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23bfe078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(x_train[0][0]), x_train[0][0])\n",
    "# 255로 나눠서 0과 1사이의 실수로 만들어야 하므로 astype() 함수를 이용해서 정수를 실수로 변환한다.\n",
    "x_train = x_train.astype('float32')\n",
    "# print(type(x_train[0][0]), x_train[0][0])\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64aa4298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "print(set(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc5c81e",
   "metadata": {},
   "source": [
    "MNIST 손글씨 데이터 분류 모델은 0부터 9까지의 숫자로 분류하는 다중 모델이므로 손실 함수로 크로스 엔트로피를 사용한다.  \n",
    "크로스 엔트로피를 계산하기 위해 실제값(y_train, y_val, y_test)을 one-hot encoding으로 변환한다.  \n",
    "one-hot encoding은 데이터를 수만은 0과 1개의 1로 데이터를 구별하는 인코딩 방식으로 0으로 이루어진 벡터 집합에 단 1개의 1의 값으로 해당 데이터를 구별하는 것을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e9a5853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# to_categorical() 함수로 데이터에 one-hot encoding을 적용한다.\n",
    "num_classes = 10\n",
    "print(y_train[:5])\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "for i in y_train[:5]:\n",
    "    print(i)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d798c87",
   "metadata": {},
   "source": [
    "<img src=\"./MNIST2.png\" width=\"900\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865d7ad",
   "metadata": {},
   "source": [
    "입력 데이터는 784개의 숫자가 들어있는 배열이다.  \n",
    "784개의 입력을 받는 256개의 노드가 1번째 히든 레이어에 있고 1번째 히든 레이어어의 출력값을 입력으로 받는 2번째 히든 레이어에는 128개의 노드가 있다.  \n",
    "2번째 히든 레이어에는 과대 적합을 방지하기 위해 10% 드롭아웃을 적용한다.  \n",
    "3번째 히든 레이어에서는 총 10개의 노드가 존재하며 이 10개의 노드값은 소프트맥스를 통과해서 0부터 9까지에 해당되는 각 숫자의 확률을 의미한다.  \n",
    "소프트맥스는 분류해야하는 정답지(레이블, 클래스, 타겟)의 총 개수를 k라고 할 때 k차원의 벡터를 입력받나 각 정답에 대한 확률을 추정한다.  \n",
    "소프트맥스의 출력값과 실제값의 차이(오차)를 계산하기 위해 크로스 엔트로피를 손실 함수로 사용하고 손실 함수를 최소화하기 위해 Adam 옵티마이저(최적화 함수)를 사용해 역전파를 통해 모든 가중치 및 편향값을 최적화한다.  \n",
    "최적화 함수 참고  \n",
    "https://onevision.tistory.com/entry/Optimizer-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%84%B1-Momentum-RMSProp-Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283b315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번째 차원이 None인 이유는 데이터 개수의 제약없이 입력받기 위해서이고 2번째 차원이 784인 것은 MNIST의 이미지 크기가 \n",
    "# 28 * 28 픽셀 = 784 픽셀이기 때문이다.\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 784]) # 학습 데이터를 기억할 placeholder를 선언한다.\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10]) # 실제값을 기억할 placeholder를 선언한다.\n",
    "keep_prob = tf.placeholder(dtype=tf.float32) # 드롭아웃 적용 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70facd8d",
   "metadata": {},
   "source": [
    "다층 퍼셉트론을 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0602889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x):\n",
    "    # 히든 레이어1\n",
    "    w1 = tf.Variable(tf.random_uniform([784, 256]))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    # 히든 레이어2\n",
    "    w2 = tf.Variable(tf.random_uniform([256, 128]))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    # 드롭아웃 적용, keep_prob만 사용했었는데 버전이 올라가면서 rate=1-keep_prob과 같은 형태로 사용해야 한다.\n",
    "    h2_drop = tf.nn.dropout(h2, rate=1-keep_prob)\n",
    "    # 히든 레이어3\n",
    "    w3 = tf.Variable(tf.random_uniform([128, 10]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    logits = tf.nn.relu(tf.matmul(h2_drop, w3) + b3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e54881",
   "metadata": {},
   "source": [
    "다층 퍼셉트론의 출력값을 logits(logistic + probit)로 정의한다. probit는 확률을 재는 단위를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd9c583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = mlp(x)\n",
    "# logits와 실제값의 크로스 엔트로피를 손실 함수로 사용한다.\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y)\n",
    ")\n",
    "# Adam 옵티마이저를 사용해 모델을 최적화한다.\n",
    "# 모델의 최적화 과정은 모델의 예측값과 실제값의 차이를 줄여나가는 과정을 의미한다.\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c139426",
   "metadata": {},
   "source": [
    "조기 종료는 과대적합을 피하고 충분한 학습을 하기 위해서 학습 중간마다 검증 데이터에 대한 정확도를 측정하면 학습 데이터에 대한 정확도는 계속 증가하는 반면 검증 데이터에 대한 정확도가 점차 떨어질 경우 학습을 중지하는 것을 말한다.  \n",
    "매 epoch 마다 검증 데이터로 검증 정확도를 측정해서 검증 정확도가 5번 연속으로 검증 정확도의 최대값보다 높지 않을 경우 조기 종료를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa685eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # 텐서플로우 변수 초기화\n",
    "saver = tf.train.Saver() # 텐서플로우에서 학습 모델의 저장 및 로드에 사용할 변수를 선언한다.\n",
    "epoch_cnt = 300 # 조기 종료가 일어나지 않을 경우 최대 300번까지 반복해서 학습하도록 설정한다.\n",
    "batch_size = 1000 # 1번에 읽어서 처리할 데이터의 개수를 설정한다.\n",
    "iteration = len(x_train) // batch_size # batch_size에 따른 학습 횟수를 설정한다.\n",
    "earlystop_threshold = 5 # 검증 정확도가 검증 정확도의 최대값보다 5번 연속으로 높지 않을 경우 조기 종료하도록 설정한다.\n",
    "earlystop_cnt = 0 # 검증 정확도가 검증 정확도의 최대값보다 연속으로 높지 않은 검증 횟수를 세는 변수를 선언한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a98882",
   "metadata": {},
   "source": [
    "학습을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f6ddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 10  5]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([3, 10, 5]) # 1차원 배열\n",
    "sess = tf.Session()\n",
    "print(sess.run(a))\n",
    "# argmax() 함수는 배열에서 가장 큰 값을 찾아서 인덱스를 리턴한다.\n",
    "# a 배열에서 10이 가장 크기 때문에 결과는 10의 인덱스인 1이 출력된다.\n",
    "print(sess.run(tf.argmax(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb9e300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 10  5]\n",
      " [ 4  5  6]\n",
      " [ 0  8  7]]\n",
      "[1 0 2]\n",
      "[1 0 2]\n",
      "[1 2 1]\n"
     ]
    }
   ],
   "source": [
    "b = tf.constant([[3, 10, 5], [4, 5, 6], [0, 8, 7]]) # 2차원 배열\n",
    "print(sess.run(b))\n",
    "# argmax() 함수를 2차원 배열에서 사용할 경우 2번째 인수를 지정해야 한다. 생략시 기본값은 0\n",
    "# 2번째 인수를 생략하거나 0을 사용하면 2차원 배열의 각 열에서 최대값의 인덱스를 리턴하고 2번째 인수에 1을 쓰면 2차원 배열의\n",
    "# 각 행에서 최대값의 인덱스를 리턴한다.\n",
    "# b 배열에서 열 단위로 최대값을 계산하면 0번째 열의 최대값 4의 인덱스 1, 1번째 열의 최대값 10의 인덱스 0, 2번째 열의 최대값\n",
    "# 7의 인덱스 2가 [1 0 2]와 같이 출력된다.\n",
    "print(sess.run(tf.argmax(b)))\n",
    "print(sess.run(tf.argmax(b, 0)))\n",
    "# b 배열의 행 단위로 최대값을 계산하면 0번째 행의 최대값 10의 인덱스 1, 1번째 행의 최대값 6의 인덱스 2, 2번째 행의 최대값\n",
    "# 8의 인덱스 1이 [1 2 1]과 같이 출력된다.\n",
    "print(sess.run(tf.argmax(b, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6ab97d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0, 학습 정확도: 0.18044, 검증 정확도: 0.18440\n",
      "epoch:   1, 학습 정확도: 0.56820, 검증 정확도: 0.58690\n",
      "epoch:   2, 학습 정확도: 0.57946, 검증 정확도: 0.60070\n",
      "epoch:   3, 학습 정확도: 0.63402, 검증 정확도: 0.65400\n",
      "epoch:   4, 학습 정확도: 0.66966, 검증 정확도: 0.69490\n",
      "epoch:   5, 학습 정확도: 0.70358, 검증 정확도: 0.72570\n",
      "epoch:   6, 학습 정확도: 0.73070, 검증 정확도: 0.74950\n",
      "epoch:   7, 학습 정확도: 0.75428, 검증 정확도: 0.77300\n",
      "epoch:   8, 학습 정확도: 0.77356, 검증 정확도: 0.79170\n",
      "epoch:   9, 학습 정확도: 0.79078, 검증 정확도: 0.80670\n",
      "epoch:  10, 학습 정확도: 0.80656, 검증 정확도: 0.82130\n",
      "epoch:  11, 학습 정확도: 0.81988, 검증 정확도: 0.83120\n",
      "epoch:  12, 학습 정확도: 0.83272, 검증 정확도: 0.84210\n",
      "epoch:  13, 학습 정확도: 0.84412, 검증 정확도: 0.84940\n",
      "epoch:  14, 학습 정확도: 0.85368, 검증 정확도: 0.85670\n",
      "epoch:  15, 학습 정확도: 0.86084, 검증 정확도: 0.86320\n",
      "epoch:  16, 학습 정확도: 0.86858, 검증 정확도: 0.87210\n",
      "epoch:  17, 학습 정확도: 0.87488, 검증 정확도: 0.87760\n",
      "epoch:  18, 학습 정확도: 0.88030, 검증 정확도: 0.88270\n",
      "epoch:  19, 학습 정확도: 0.88506, 검증 정확도: 0.88680\n",
      "epoch:  20, 학습 정확도: 0.89068, 검증 정확도: 0.89240\n",
      "epoch:  21, 학습 정확도: 0.89448, 검증 정확도: 0.89610\n",
      "epoch:  22, 학습 정확도: 0.89826, 검증 정확도: 0.89920\n",
      "epoch:  23, 학습 정확도: 0.90084, 검증 정확도: 0.90100\n",
      "epoch:  24, 학습 정확도: 0.90542, 검증 정확도: 0.90470\n",
      "epoch:  25, 학습 정확도: 0.90868, 검증 정확도: 0.90660\n",
      "epoch:  26, 학습 정확도: 0.91144, 검증 정확도: 0.90850\n",
      "epoch:  27, 학습 정확도: 0.91374, 검증 정확도: 0.90960\n",
      "epoch:  28, 학습 정확도: 0.91644, 검증 정확도: 0.91160\n",
      "epoch:  29, 학습 정확도: 0.91832, 검증 정확도: 0.91250\n",
      "epoch:  30, 학습 정확도: 0.92154, 검증 정확도: 0.91490\n",
      "epoch:  31, 학습 정확도: 0.92300, 검증 정확도: 0.91580\n",
      "epoch:  32, 학습 정확도: 0.92538, 검증 정확도: 0.91740\n",
      "epoch:  33, 학습 정확도: 0.92762, 검증 정확도: 0.91870\n",
      "epoch:  34, 학습 정확도: 0.92922, 검증 정확도: 0.92010\n",
      "epoch:  35, 학습 정확도: 0.93106, 검증 정확도: 0.92140\n",
      "epoch:  36, 학습 정확도: 0.93210, 검증 정확도: 0.92190\n",
      "epoch:  37, 학습 정확도: 0.93368, 검증 정확도: 0.92300\n",
      "epoch:  38, 학습 정확도: 0.93516, 검증 정확도: 0.92420\n",
      "epoch:  39, 학습 정확도: 0.93608, 검증 정확도: 0.92510\n",
      "epoch:  40, 학습 정확도: 0.93722, 검증 정확도: 0.92570\n",
      "epoch:  41, 학습 정확도: 0.93852, 검증 정확도: 0.92750\n",
      "epoch:  42, 학습 정확도: 0.94018, 검증 정확도: 0.92770\n",
      "epoch:  43, 학습 정확도: 0.94150, 검증 정확도: 0.92960\n",
      "epoch:  44, 학습 정확도: 0.94192, 검증 정확도: 0.93030\n",
      "epoch:  45, 학습 정확도: 0.94312, 검증 정확도: 0.93170\n",
      "epoch:  46, 학습 정확도: 0.94452, 검증 정확도: 0.93210\n",
      "epoch:  47, 학습 정확도: 0.94490, 검증 정확도: 0.93340\n",
      "epoch:  48, 학습 정확도: 0.94612, 검증 정확도: 0.93380\n",
      "epoch:  49, 학습 정확도: 0.94730, 검증 정확도: 0.93460\n",
      "epoch:  50, 학습 정확도: 0.94836, 검증 정확도: 0.93590\n",
      "epoch:  51, 학습 정확도: 0.94940, 검증 정확도: 0.93540\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  52, 학습 정확도: 0.95006, 검증 정확도: 0.93560\n",
      "과대적합 경고 횟수: 2\n",
      "epoch:  53, 학습 정확도: 0.95108, 검증 정확도: 0.93760\n",
      "epoch:  54, 학습 정확도: 0.95252, 검증 정확도: 0.93900\n",
      "epoch:  55, 학습 정확도: 0.95256, 검증 정확도: 0.93870\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  56, 학습 정확도: 0.95380, 검증 정확도: 0.93880\n",
      "과대적합 경고 횟수: 2\n",
      "epoch:  57, 학습 정확도: 0.95464, 검증 정확도: 0.94080\n",
      "epoch:  58, 학습 정확도: 0.95556, 검증 정확도: 0.93960\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  59, 학습 정확도: 0.95676, 검증 정확도: 0.94040\n",
      "과대적합 경고 횟수: 2\n",
      "epoch:  60, 학습 정확도: 0.95684, 검증 정확도: 0.93990\n",
      "과대적합 경고 횟수: 3\n",
      "epoch:  61, 학습 정확도: 0.95800, 검증 정확도: 0.94010\n",
      "과대적합 경고 횟수: 4\n",
      "epoch:  62, 학습 정확도: 0.95892, 검증 정확도: 0.94080\n",
      "epoch:  63, 학습 정확도: 0.95944, 검증 정확도: 0.94170\n",
      "epoch:  64, 학습 정확도: 0.96040, 검증 정확도: 0.94170\n",
      "epoch:  65, 학습 정확도: 0.96074, 검증 정확도: 0.94180\n",
      "epoch:  66, 학습 정확도: 0.96122, 검증 정확도: 0.94200\n",
      "epoch:  67, 학습 정확도: 0.96206, 검증 정확도: 0.94170\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  68, 학습 정확도: 0.96290, 검증 정확도: 0.94300\n",
      "epoch:  69, 학습 정확도: 0.96366, 검증 정확도: 0.94390\n",
      "epoch:  70, 학습 정확도: 0.96412, 검증 정확도: 0.94300\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  71, 학습 정확도: 0.96426, 검증 정확도: 0.94330\n",
      "과대적합 경고 횟수: 2\n",
      "epoch:  72, 학습 정확도: 0.96568, 검증 정확도: 0.94430\n",
      "epoch:  73, 학습 정확도: 0.96554, 검증 정확도: 0.94410\n",
      "epoch:  74, 학습 정확도: 0.96606, 검증 정확도: 0.94490\n",
      "epoch:  75, 학습 정확도: 0.96686, 검증 정확도: 0.94530\n",
      "epoch:  76, 학습 정확도: 0.96658, 검증 정확도: 0.94360\n",
      "epoch:  77, 학습 정확도: 0.96778, 검증 정확도: 0.94560\n",
      "epoch:  78, 학습 정확도: 0.96764, 검증 정확도: 0.94450\n",
      "epoch:  79, 학습 정확도: 0.96864, 검증 정확도: 0.94580\n",
      "epoch:  80, 학습 정확도: 0.96826, 검증 정확도: 0.94560\n",
      "epoch:  81, 학습 정확도: 0.96998, 검증 정확도: 0.94640\n",
      "epoch:  82, 학습 정확도: 0.97056, 검증 정확도: 0.94640\n",
      "epoch:  83, 학습 정확도: 0.97042, 검증 정확도: 0.94730\n",
      "epoch:  84, 학습 정확도: 0.97060, 검증 정확도: 0.94710\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  85, 학습 정확도: 0.97134, 검증 정확도: 0.94760\n",
      "epoch:  86, 학습 정확도: 0.97254, 검증 정확도: 0.94730\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  87, 학습 정확도: 0.97224, 검증 정확도: 0.94760\n",
      "epoch:  88, 학습 정확도: 0.97276, 검증 정확도: 0.94720\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  89, 학습 정확도: 0.97292, 검증 정확도: 0.94950\n",
      "epoch:  90, 학습 정확도: 0.97422, 검증 정확도: 0.95040\n",
      "epoch:  91, 학습 정확도: 0.97434, 검증 정확도: 0.94890\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  92, 학습 정확도: 0.97488, 검증 정확도: 0.94850\n",
      "과대적합 경고 횟수: 2\n",
      "epoch:  93, 학습 정확도: 0.97444, 검증 정확도: 0.94970\n",
      "epoch:  94, 학습 정확도: 0.97472, 검증 정확도: 0.94980\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  95, 학습 정확도: 0.97550, 검증 정확도: 0.94940\n",
      "과대적합 경고 횟수: 2\n",
      "epoch:  96, 학습 정확도: 0.97610, 검증 정확도: 0.95040\n",
      "epoch:  97, 학습 정확도: 0.97676, 검증 정확도: 0.94970\n",
      "과대적합 경고 횟수: 1\n",
      "epoch:  98, 학습 정확도: 0.97718, 검증 정확도: 0.95010\n",
      "과대적합 경고 횟수: 2\n",
      "epoch:  99, 학습 정확도: 0.97678, 검증 정확도: 0.95090\n",
      "epoch: 100, 학습 정확도: 0.97894, 검증 정확도: 0.95180\n",
      "epoch: 101, 학습 정확도: 0.97882, 검증 정확도: 0.95150\n",
      "epoch: 102, 학습 정확도: 0.97836, 검증 정확도: 0.95070\n",
      "epoch: 103, 학습 정확도: 0.97892, 검증 정확도: 0.95010\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 104, 학습 정확도: 0.97940, 검증 정확도: 0.95310\n",
      "epoch: 105, 학습 정확도: 0.97958, 검증 정확도: 0.95130\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 106, 학습 정확도: 0.97870, 검증 정확도: 0.95080\n",
      "epoch: 107, 학습 정확도: 0.97924, 검증 정확도: 0.94970\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 108, 학습 정확도: 0.98006, 검증 정확도: 0.95150\n",
      "과대적합 경고 횟수: 2\n",
      "epoch: 109, 학습 정확도: 0.98040, 검증 정확도: 0.95020\n",
      "과대적합 경고 횟수: 3\n",
      "epoch: 110, 학습 정확도: 0.97958, 검증 정확도: 0.95100\n",
      "epoch: 111, 학습 정확도: 0.98096, 검증 정확도: 0.95230\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 112, 학습 정확도: 0.98094, 검증 정확도: 0.95320\n",
      "epoch: 113, 학습 정확도: 0.98110, 검증 정확도: 0.95200\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 114, 학습 정확도: 0.98046, 검증 정확도: 0.95070\n",
      "epoch: 115, 학습 정확도: 0.98198, 검증 정확도: 0.95200\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 116, 학습 정확도: 0.98222, 검증 정확도: 0.95160\n",
      "과대적합 경고 횟수: 2\n",
      "epoch: 117, 학습 정확도: 0.98296, 검증 정확도: 0.95330\n",
      "epoch: 118, 학습 정확도: 0.98196, 검증 정확도: 0.95180\n",
      "epoch: 119, 학습 정확도: 0.98348, 검증 정확도: 0.95250\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 120, 학습 정확도: 0.98266, 검증 정확도: 0.95110\n",
      "epoch: 121, 학습 정확도: 0.98276, 검증 정확도: 0.95260\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 122, 학습 정확도: 0.98064, 검증 정확도: 0.95000\n",
      "epoch: 123, 학습 정확도: 0.98050, 검증 정확도: 0.95070\n",
      "epoch: 124, 학습 정확도: 0.98320, 검증 정확도: 0.94990\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 125, 학습 정확도: 0.98168, 검증 정확도: 0.95230\n",
      "epoch: 126, 학습 정확도: 0.98376, 검증 정확도: 0.95310\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 127, 학습 정확도: 0.98248, 검증 정확도: 0.95390\n",
      "epoch: 128, 학습 정확도: 0.98314, 검증 정확도: 0.95250\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 129, 학습 정확도: 0.98358, 검증 정확도: 0.95280\n",
      "과대적합 경고 횟수: 2\n",
      "epoch: 130, 학습 정확도: 0.98460, 검증 정확도: 0.95350\n",
      "과대적합 경고 횟수: 3\n",
      "epoch: 131, 학습 정확도: 0.98308, 검증 정확도: 0.95180\n",
      "epoch: 132, 학습 정확도: 0.98344, 검증 정확도: 0.95240\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 133, 학습 정확도: 0.98308, 검증 정확도: 0.95310\n",
      "epoch: 134, 학습 정확도: 0.98390, 검증 정확도: 0.95300\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 135, 학습 정확도: 0.98356, 검증 정확도: 0.95420\n",
      "epoch: 136, 학습 정확도: 0.98416, 검증 정확도: 0.95290\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 137, 학습 정확도: 0.98582, 검증 정확도: 0.95380\n",
      "과대적합 경고 횟수: 2\n",
      "epoch: 138, 학습 정확도: 0.98540, 검증 정확도: 0.95420\n",
      "epoch: 139, 학습 정확도: 0.98636, 검증 정확도: 0.95590\n",
      "epoch: 140, 학습 정확도: 0.98408, 검증 정확도: 0.95570\n",
      "epoch: 141, 학습 정확도: 0.98066, 검증 정확도: 0.95070\n",
      "epoch: 142, 학습 정확도: 0.98422, 검증 정확도: 0.95460\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 143, 학습 정확도: 0.98526, 검증 정확도: 0.95460\n",
      "과대적합 경고 횟수: 2\n",
      "epoch: 144, 학습 정확도: 0.98534, 검증 정확도: 0.95170\n",
      "과대적합 경고 횟수: 3\n",
      "epoch: 145, 학습 정확도: 0.98654, 검증 정확도: 0.95420\n",
      "과대적합 경고 횟수: 4\n",
      "epoch: 146, 학습 정확도: 0.98596, 검증 정확도: 0.95330\n",
      "epoch: 147, 학습 정확도: 0.98648, 검증 정확도: 0.95530\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 148, 학습 정확도: 0.98642, 검증 정확도: 0.95610\n",
      "epoch: 149, 학습 정확도: 0.98374, 검증 정확도: 0.95210\n",
      "epoch: 150, 학습 정확도: 0.98376, 검증 정확도: 0.95390\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 151, 학습 정확도: 0.98550, 검증 정확도: 0.95560\n",
      "과대적합 경고 횟수: 2\n",
      "epoch: 152, 학습 정확도: 0.98562, 검증 정확도: 0.95580\n",
      "과대적합 경고 횟수: 3\n",
      "epoch: 153, 학습 정확도: 0.98480, 검증 정확도: 0.95450\n",
      "epoch: 154, 학습 정확도: 0.98474, 검증 정확도: 0.95420\n",
      "epoch: 155, 학습 정확도: 0.98270, 검증 정확도: 0.95300\n",
      "epoch: 156, 학습 정확도: 0.98374, 검증 정확도: 0.95430\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 157, 학습 정확도: 0.98662, 검증 정확도: 0.95700\n",
      "epoch: 158, 학습 정확도: 0.98766, 검증 정확도: 0.95500\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 159, 학습 정확도: 0.98822, 검증 정확도: 0.95650\n",
      "과대적합 경고 횟수: 2\n",
      "epoch: 160, 학습 정확도: 0.98898, 검증 정확도: 0.95630\n",
      "과대적합 경고 횟수: 3\n",
      "epoch: 161, 학습 정확도: 0.98808, 검증 정확도: 0.95610\n",
      "epoch: 162, 학습 정확도: 0.98682, 검증 정확도: 0.95680\n",
      "epoch: 163, 학습 정확도: 0.98824, 검증 정확도: 0.95700\n",
      "epoch: 164, 학습 정확도: 0.98828, 검증 정확도: 0.95760\n",
      "epoch: 165, 학습 정확도: 0.98832, 검증 정확도: 0.95740\n",
      "과대적합 경고 횟수: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 166, 학습 정확도: 0.98960, 검증 정확도: 0.95930\n",
      "epoch: 167, 학습 정확도: 0.98802, 검증 정확도: 0.95820\n",
      "epoch: 168, 학습 정확도: 0.98856, 검증 정확도: 0.95740\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 169, 학습 정확도: 0.98984, 검증 정확도: 0.95740\n",
      "과대적합 경고 횟수: 2\n",
      "epoch: 170, 학습 정확도: 0.98766, 검증 정확도: 0.95600\n",
      "epoch: 171, 학습 정확도: 0.98976, 검증 정확도: 0.95700\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 172, 학습 정확도: 0.98780, 검증 정확도: 0.95700\n",
      "epoch: 173, 학습 정확도: 0.98680, 검증 정확도: 0.95590\n",
      "epoch: 174, 학습 정확도: 0.98664, 검증 정확도: 0.95650\n",
      "epoch: 175, 학습 정확도: 0.98876, 검증 정확도: 0.95440\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 176, 학습 정확도: 0.98770, 검증 정확도: 0.95580\n",
      "epoch: 177, 학습 정확도: 0.98598, 검증 정확도: 0.95610\n",
      "epoch: 178, 학습 정확도: 0.98858, 검증 정확도: 0.95900\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 179, 학습 정확도: 0.98640, 검증 정확도: 0.95560\n",
      "epoch: 180, 학습 정확도: 0.98956, 검증 정확도: 0.95940\n",
      "epoch: 181, 학습 정확도: 0.98718, 검증 정확도: 0.95540\n",
      "epoch: 182, 학습 정확도: 0.99000, 검증 정확도: 0.95740\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 183, 학습 정확도: 0.98874, 검증 정확도: 0.95860\n",
      "epoch: 184, 학습 정확도: 0.98838, 검증 정확도: 0.95740\n",
      "epoch: 185, 학습 정확도: 0.98872, 검증 정확도: 0.95550\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 186, 학습 정확도: 0.99042, 검증 정확도: 0.96180\n",
      "epoch: 187, 학습 정확도: 0.99176, 검증 정확도: 0.96110\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 188, 학습 정확도: 0.99138, 검증 정확도: 0.96240\n",
      "epoch: 189, 학습 정확도: 0.98950, 검증 정확도: 0.95970\n",
      "epoch: 190, 학습 정확도: 0.99052, 검증 정확도: 0.95940\n",
      "과대적합 경고 횟수: 1\n",
      "epoch: 191, 학습 정확도: 0.99058, 검증 정확도: 0.96100\n",
      "과대적합 경고 횟수: 2\n",
      "epoch: 192, 학습 정확도: 0.99132, 검증 정확도: 0.96140\n",
      "과대적합 경고 횟수: 3\n",
      "epoch: 193, 학습 정확도: 0.99092, 검증 정확도: 0.96030\n",
      "과대적합 경고 횟수: 4\n",
      "epoch: 194, 학습 정확도: 0.99204, 검증 정확도: 0.96180\n",
      "과대적합 경고 횟수: 5\n",
      "epoch: 195, 학습 정확도: 0.99076, 검증 정확도: 0.96030\n",
      "조기 종료 시점: 195\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    prev_train_acc = 0.0 # 이전 학습 정확도를 기억할 변수를 선언한다.\n",
    "    max_val_acc = 0.0 # 검증 정확도의 최대값을 기억할 변수를 선언한다.\n",
    "    # 지정한 최대 epoch 만큼 학습한다. => 검증 정확도가 검증 정확도의 최대값보다 5번 연속으로 높지 않을 경우 조기 종료한다.\n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.0 # 손실값\n",
    "        start = 0 # 학습 시작 위치\n",
    "        end = batch_size # 학습 끝 위치\n",
    "        # 학습 데이터를 batch_size 개 만큼씩 나눠서 학습을 진행한다.\n",
    "        for i in range(iteration):\n",
    "            _, loss_op = sess.run([train, loss], feed_dict={x: x_train[start:end], y: y_train[start:end], keep_prob: 0.9})\n",
    "            # 학습할 데이터의 범위를 batch_size 만큼 이동시킨다.\n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "            # 크로스 엔트로피 손실 함수의 학습 손실값을 계산한다.\n",
    "            avg_loss += loss_op / iteration\n",
    "        # ===== for i\n",
    "        \n",
    "        # 모델 검증\n",
    "        predict = tf.nn.softmax(logits=logits) # 소프트맥스 적용\n",
    "        correct_prediction = tf.equal(tf.argmax(predict, 1), tf.argmax(y, 1))\n",
    "        \n",
    "        # 정확도 계산\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "        # 학습 정확도 계산\n",
    "        # Session.run()과 Tansor.eval()의 차이\n",
    "        # t가 Trnsor 오브젝트라면 t.eval()은 sess.run(t)의 속기 표현이다. sess가 현재 디폴트 세션인 곳에서만 가능하다.\n",
    "        cur_train_acc = accuracy.eval({x: x_train, y: y_train, keep_prob: 1.0})\n",
    "        # 검증 정확도 계산\n",
    "        cur_val_acc = accuracy.eval({x: x_val, y: y_val, keep_prob: 1.0})\n",
    "        # 검증 데이터에 대한 손실값 계산\n",
    "        cur_val_loss = loss.eval({x: x_val, y: y_val, keep_prob: 1.0})\n",
    "        # 학습 정확도와 검증 정확도를 출력한다.\n",
    "        print('epoch: {:3d}, 학습 정확도: {:7.5f}, 검증 정확도: {:7.5f}'.format(epoch, cur_train_acc, cur_val_acc))\n",
    "        \n",
    "        # 현재 검증 정확도와 검증 정확도의 최대값을 비교한다.\n",
    "        if cur_val_acc < max_val_acc:\n",
    "            # 현재 검증 정확도가 검증 정확도의 최대값 미만이면\n",
    "            # 현재 학습 정확도와 이전 학습 정확도, 현재 학습 정확도와 0.99를 비교한다.\n",
    "            if cur_train_acc > prev_train_acc or cur_train_acc > 0.99:\n",
    "                # 현재 학습 정확도가 이전 학습 정확도 보다 크거나 현재 학습 정확도가 0.99 보다 크면\n",
    "                if earlystop_cnt == earlystop_threshold:\n",
    "                    print('조기 종료 시점: {}'.format(epoch))\n",
    "                    break\n",
    "                else:\n",
    "                    # 5번 연속으로 현재 학습 정확도가 이전 학습 정확도 보다 크지 않다면 이전 학습 정확도가 현재 학습 정확도\n",
    "                    # 보다 크므로 현재 검증 정확도가 검증 정확도의 최대값보다 연속으로 높지 않은 횟수를 카운트 하는 변수\n",
    "                    # earlystop_cnt를 1증가 시킨다.\n",
    "                    earlystop_cnt += 1\n",
    "                    print('과대적합 경고 횟수: {}'.format(earlystop_cnt))\n",
    "                # ===== if\n",
    "            else:\n",
    "                # 현재 학습 정확도가 이전 학습 정확도 이하 이거나 현재 학습 정확도가 0.99 보다 이하이면\n",
    "                # 이전 학습 정확도가 현재 학습 정확도 이상이므로 현재 검증 정확도가 검증 정확도의 최대값보다 연속으로\n",
    "                # 높지 않은 횟수를 카운트 하는 변수를 0으로 초기화시킨다.\n",
    "                earlystop_cnt = 0\n",
    "            # ===== if\n",
    "        else:\n",
    "            # 현재 검증 정확도가 검증 정확도의 최대값 이상이면\n",
    "            # 현재 검증 정확도가 검증 정확도의 최대값 보다 연속을 높지 않은 횟수를 기억하는 변수 earlystop_cnt를 \n",
    "            # 0으로 초기화시킨다.\n",
    "            earlystop_cnt = 0\n",
    "            # 검증 정확도의 최대값을 현재 검증 정확도로 교체한다.\n",
    "            max_val_acc = cur_val_acc\n",
    "            # 검증 정확도가 가장 높은 현재 모델을 저장한다.\n",
    "            save_path = saver.save(sess, './model/model.ckpt')\n",
    "        # ===== if\n",
    "        # 다음 학습을 위해 현재 학습 정확도를 이전 학습 정확도를 기억하는 변수에 넣어준다.\n",
    "        prev_train_acc = cur_train_acc\n",
    "    # ===== for\n",
    "# ===== with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e50a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53301526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c845c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496278d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
